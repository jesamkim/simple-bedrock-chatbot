import random
import json
from typing import List, Tuple, Union, Dict
import os
import tempfile
import csv
import pandas as pd
import warnings

# Pydantic ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

import streamlit as st
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_aws import ChatBedrock
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import boto3

# MCP ê¸°ëŠ¥ ì„í¬íŠ¸
import re
import json
from mcp_client import UnifiedMCPClient

# í†µí•© MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
mcp_client = UnifiedMCPClient()
extract_keywords = mcp_client.extract_keywords

# ì§ˆì˜ ì˜ë„ íƒ€ì…
class QueryIntent:
    DATETIME = "datetime"     # ë‚ ì§œ/ì‹œê°„ ê´€ë ¨ ì§ˆì˜
    SEARCH = "search"         # ê²€ìƒ‰ì´ í•„ìš”í•œ ì§ˆì˜
    GENERAL = "general"       # íŠ¹ì • íŒ¨í„´ì´ ì—†ëŠ” ì¼ë°˜ ì§ˆì˜
    MIXED = "mixed"           # ë‚ ì§œ/ì‹œê°„ê³¼ ê²€ìƒ‰ ëª¨ë‘ í•„ìš”í•œ ë³µí•© ì˜ë„
    TIME_COMPARISON = "time_comparison"  # ì‹œê°„ ë¹„êµ (ê¸°ê°„, ê²½ê³¼ ì‹œê°„ ë“±)

# íŒ¨í„´ ì •ì˜
# ì‹œê°„ ê´€ë ¨ íŒ¨í„´ - ëª…í™•í•œ ì‹œê°„ ì§ˆì˜ì—ë§Œ ë§¤ì¹˜ë˜ë„ë¡ êµ¬ì²´í™”
TIME_PATTERNS = [
    r'\b(ì§€ê¸ˆ|í˜„ì¬|ì˜¤ëŠ˜)?\s*(ëª‡\s*ì‹œ|ì‹œê°„|ì‹œê³„)\b',
    r'\b(what|current)\s*time\b',
    r'\btime\s*(now|is it)\b'
]

# ë‚ ì§œ ê´€ë ¨ íŒ¨í„´ - ëª…í™•í•œ ë‚ ì§œ ì§ˆì˜ì—ë§Œ ë§¤ì¹˜ë˜ë„ë¡ êµ¬ì²´í™”
DATE_PATTERNS = [
    r'\b(ì˜¤ëŠ˜|ì§€ê¸ˆ|í˜„ì¬)\s*(ë¬´ìŠ¨|ë©°ì¹ |ëª‡\s*ì¼|ë‚ ì§œ)\b',
    r'\b(ë¬´ìŠ¨|ë©°ì¹ |ëª‡\s*ì¼|ë‚ ì§œ)\b',
    r'\b(what|current)\s*(day|date)\b',
    r'\bdate\s*today\b',
    r'\btoday\s*is\b'
]

# ì‹œê°„ ë¹„êµ íŒ¨í„´ (ë‘ ì‹œê°„/ë‚ ì§œ ê°„ì˜ ê´€ê³„ ì§ˆì˜)
TIME_COMPARISON_PATTERNS = [
    r'\b(ì–¼ë§ˆë‚˜|ë©°ì¹ ì´|ëª‡\s*ì¼ì´|ëª‡\s*ë…„ì´|ëª‡\s*ê°œì›”ì´|ì‹œê°„ì´)\s*(ì§€ë‚¬|ê²½ê³¼|ë|í˜ë €|ë‚¨ì•˜|ì°¨ì´|ë²Œì–´)\w*\b',
    r'\b(ë¶€í„°|ì´í›„|ë¡œë¶€í„°|ì „ë¶€í„°|ì´ì „ë¶€í„°|ì´ë˜|ê¹Œì§€)\s*(ì–¼ë§ˆë‚˜|ëª‡|ì§€ë‚œ)\w*\b',
    r'\b(ëª‡\s*ì¼\s*ì „|ëª‡\s*ì¼\s*í›„|ëª‡\s*ì¼\s*ë’¤)\b',
    r'\b(ê¸°ê°„|ê°„ê²©|ë‚ ì§œ\s*ì°¨ì´|ì‹œê°„\s*ì°¨ì´)\b',
    r'\b(since|elapsed|passed|ago|difference)\b'
]

# í˜„ì¬ ì‹œê°„/ë‚ ì§œ ê´€ë ¨ íŒ¨í„´ (ë‚ ì§œ/ì‹œê°„ ê³„ì‚° ì‹œ í˜„ì¬ ì‹œì  ì§€ì‹œ)
CURRENT_TIME_PATTERNS = [
    r'\b(ì˜¤ëŠ˜|ì§€ê¸ˆ|í˜„ì¬|ì´ì œ|now|today|current)\b'
]

# ì£¼ì˜í•´ì•¼ í•  í˜¼ë™ ê°€ëŠ¥ ë‹¨ì–´ (ì´ë“¤ì´ í¬í•¨ë˜ë©´ ë‚ ì§œ/ì‹œê°„ ì§ˆì˜ê°€ ì•„ë‹ ê°€ëŠ¥ì„± ë†’ìŒ)
AMBIGUOUS_TERMS = [
    r'\b(ì¶œì‹œì¼|ìƒì¼|ê¸°ë…ì¼|ê³µíœ´ì¼|ë°œë§¤ì¼|ë“±ë¡ì¼|ì‹œì‘ì¼|ë§Œê¸°ì¼)\b',
    r'\b(ìƒë…„ì›”ì¼|ì„¤ë¦½ì¼|ê³„ì•½ì¼|ì…ì‚¬ì¼|í‡´ì‚¬ì¼)\b'
]

# ë‚ ì§œ/ì‹œê°„ ë³µí•© íŒ¨í„´ (ì¢€ ë” êµ¬ì²´ì ì¸ íŒ¨í„´ë§Œ í¬í•¨)
DATETIME_PATTERNS = [
    r'\bì§€ê¸ˆ|í˜„ì¬\s*(ì‹œê°„|ë‚ ì§œ)\b',
    r'\bnow\b'
]

# ê²€ìƒ‰ ì˜ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŒ¨í„´
SEARCH_PATTERNS = [
    r'\b(ë­|ë¬´ì—‡|ì–´ë–¤|ì–´ë””|ì–¸ì œ|ì™œ|ëˆ„êµ¬|ì–´ë–»ê²Œ)\b',
    r'\b(ì•Œë ¤|ì°¾ì•„|ê²€ìƒ‰|ì˜ë¯¸|ëœ»|ë°©ë²•|ê°€ê²©|ìœ„ì¹˜|ëˆ„êµ¬ì˜|ì–´ë””ì˜)\b',
    r'\b(ì •ë³´|ì‚¬ìš©ë²•|ì°¨ì´|ë¹„êµ|ì¢…ë¥˜|ë¬¸ì œ|ì´ìœ |ë°œë§¤|ì‚¬ì–‘)\b',
    r'\b(what|where|when|why|who|how)\b',
    r'\b(information|details|compare|difference|release|reason)\b'
]

REGION = os.getenv("AWS_REGION", "us-west-2")
if not REGION:
    raise ValueError("AWS_REGION í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

MODELS = {
    "Claude 3.7 Sonnet": {
        "id": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "class": ChatBedrock,
        "use_model_kwargs": True,
        "max_tokens": 8192
    }
}

SUPPORTED_FORMATS = ['pdf', 'doc', 'docx', 'md', 'ppt', 'pptx', 'txt', 'html', 'csv', 'xls', 'xlsx']
MAX_MESSAGES = 10  # ëŒ€í™” ê¸°ë¡ ìµœëŒ€ ìœ ì§€ ìˆ˜

class ChatMessage:
    def __init__(self, role: str, text: str):
        self.role = role
        self.text = text

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "widget_key" not in st.session_state:
        st.session_state["widget_key"] = str(random.randint(1, 1000000))
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = StreamlitChatMessageHistory(key="chat_history")
    if "context" not in st.session_state:
        st.session_state.context = None
    if "initial_system_message" not in st.session_state:
        st.session_state.initial_system_message = None

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container: st.container) -> None:
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

def set_page_config() -> None:
    st.set_page_config(page_title="Bedrock Chatbot", layout="wide")
    st.title("Bedrock Chatbot with Document Q&A")

def get_sidebar_params() -> Tuple[float, float, int, int, int, str, object, str, bool, bool, bool]:
    with st.sidebar:
        st.markdown("## ëª¨ë¸ ì •ë³´")
        # ëª¨ë¸ ì„ íƒ ëŒ€ì‹  ê³ ì • í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
        st.info("Claude 3.7 Sonnet v1")
        
        # ëª¨ë¸ ì´ë¦„ì€ í•­ìƒ ê³ ì •
        model_name = "Claude 3.7 Sonnet"
        
        # ëª¨ë“œ ì„ íƒ (ê¸°ë³¸, MCP, Reasoning ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒ ê°€ëŠ¥)
        mode = st.radio(
            "ì‘ë™ ëª¨ë“œ ì„ íƒ",
            options=["ê¸°ë³¸ ëª¨ë“œ", "MCP ëª¨ë“œ", "Reasoning ëª¨ë“œ"],
            index=1,
            help="ê¸°ë³¸ ëª¨ë“œ: ì¼ë°˜ ì±—ë´‡ ê¸°ëŠ¥, MCP ëª¨ë“œ: ì›¹ ê²€ìƒ‰ ë° í˜„ì¬ ì‹œê°„/ë‚ ì§œ ì •ë³´ ì œê³µ, Reasoning ëª¨ë“œ: ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì— íŠ¹í™”ëœ ì‚¬ê³  ê³¼ì • ì œê³µ",
            key=f"{st.session_state['widget_key']}_Mode"
        )
        
        # ì„ íƒëœ ëª¨ë“œì— ë”°ë¼ í”Œë˜ê·¸ ì„¤ì •
        extended_thinking = False
        show_reasoning = False
        mcp_enable = False
        
        if mode == "Reasoning ëª¨ë“œ":
            extended_thinking = True
            show_reasoning = True
            st.info("Reasoning ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ Temperature ê°’ì´ 1.0ìœ¼ë¡œ ìë™ ì„¤ì •ë˜ê³  Top-K ë° Top-P ì„¤ì •ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
            
            # Reasoning ëª¨ë“œì—ì„œë§Œ í‘œì‹œ ì—¬ë¶€ ì˜µì…˜ ì œê³µ
            show_reasoning = st.checkbox(
                "Reasoning ê³¼ì • í‘œì‹œ",
                value=True,
                help="Claudeì˜ ì‚¬ê³  ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.",
                key=f"{st.session_state['widget_key']}_Show_Reasoning"
            )
        elif mode == "MCP ëª¨ë“œ":
            mcp_enable = True
            st.info("MCP ëª¨ë“œì—ì„œëŠ” ì›¹ ê²€ìƒ‰ ë° ì‹œê°„/ë‚ ì§œ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.")
        
        st.markdown("## Document Upload")
        uploaded_file = st.file_uploader(
            "Upload a document",
            type=SUPPORTED_FORMATS,
            help="ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: pdf, doc, docx, md, ppt, pptx, txt, html, csv, xls, xlsx",
            key=f"{st.session_state['widget_key']}_file_uploader"  # widget_keyì™€ ì—°ë™
        )

        st.markdown("## Inference Parameters")
        system_prompt = st.text_area(
            "System Prompt",
            "You're a helpful assistant who loves to respond in Korean. You'll answer questions based on the uploaded documents, when a document exists. When you're not certain about something, don't guess or make up information. Instead, honestly admit that you don't know.",
            key=f"{st.session_state['widget_key']}_System_Prompt",
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.1,
            key=f"{st.session_state['widget_key']}_Temperature",
        )

        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                top_p = st.slider(
                    "Top-P",
                    min_value=0.0,
                    max_value=1.0,
                    value=1.00,
                    step=0.01,
                    key=f"{st.session_state['widget_key']}_Top-P",
                )
            with col2:
                top_k = st.slider(
                    "Top-K",
                    min_value=1,
                    max_value=500,
                    value=500,
                    step=5,
                    key=f"{st.session_state['widget_key']}_Top-K",
                )

        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                model_max_tokens = MODELS[model_name]["max_tokens"]
                max_tokens = st.slider(
                    "Max Token",
                    min_value=0,
                    max_value=model_max_tokens,
                    value=model_max_tokens,
                    step=8,
                    key=f"{st.session_state['widget_key']}_Max_Token",
                )
            with col2:
                memory_window = st.slider(
                    "Memory Window",
                    min_value=0,
                    max_value=10,
                    value=10,
                    step=1,
                    key=f"{st.session_state['widget_key']}_Memory_Window",
                )

    return temperature, top_p, top_k, max_tokens, memory_window, system_prompt, uploaded_file, model_name, extended_thinking, show_reasoning, mcp_enable

def process_uploaded_file(file_path: str) -> str:
    """ë¬¸ì„œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_ext == '.pdf':
            from PyPDF2 import PdfReader
            reader = PdfReader(file_path)
            # PDF íŒŒì¼ì´ ì•”í˜¸í™”ë˜ì–´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ë˜ì§€ ì•ŠìŒ
            if reader.is_encrypted:
                raise ValueError("ì•”í˜¸í™”ëœ PDF íŒŒì¼ì€ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text
        
        elif file_ext in ['.doc', '.docx']:
            from docx import Document
            doc = Document(file_path)
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        
        elif file_ext in ['.txt', '.md', '.html']:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        
        elif file_ext in ['.csv']:
            df = pd.read_csv(file_path)
            return df.to_string()
        
        elif file_ext in ['.xls', '.xlsx']:
            df = pd.read_excel(file_path)
            return df.to_string()
        
        elif file_ext in ['.ppt', '.pptx']:
            from pptx import Presentation
            text = []
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text.append(shape.text)
            return "\n\n".join(text)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_ext}")
            
    except Exception as e:
        raise Exception(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({file_ext}): {str(e)}")

def init_conversation_chain(
    temperature: float,
    top_p: float,
    top_k: int,
    max_tokens: int,
    system_prompt: str,
    model_name: str,
    extended_thinking: bool = False,
) -> Tuple[boto3.client, dict]:

    model_info = MODELS[model_name]
    model_id = model_info["id"]
    
    # ì§ì ‘ boto3 í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
    bedrock_client = boto3.client("bedrock-runtime", region_name=REGION)
    
    model_params = {
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "max_tokens": max_tokens,
        "system": system_prompt,
        "model_id": model_id,
    }
    
    # Model reasoning ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°
    if extended_thinking:
        # Model reasoning ëª¨ë“œì—ì„œëŠ” temperatureê°€ ë°˜ë“œì‹œ 1ì´ì–´ì•¼ í•¨
        model_params["temperature"] = 1.0
        model_params["anthropic_version"] = "bedrock-2023-05-31"
        
        # Model reasoning ëª¨ë“œì—ì„œëŠ” top_kì™€ top_pë¥¼ ì„¤ì •í•˜ì§€ ì•Šì•„ì•¼ í•¨
        if "top_k" in model_params:
            del model_params["top_k"]
        if "top_p" in model_params:
            del model_params["top_p"]
        
        # ìµœëŒ€ LengthëŠ” 64000ìœ¼ë¡œ ì„¤ì •
        model_params["max_tokens"] = 64000
        
        # thinking.budget_tokensëŠ” max_tokensë³´ë‹¤ ì‘ì•„ì•¼ í•˜ë©°, ìµœëŒ€ 4096
        thinking_budget = min(4096, model_params["max_tokens"] - 1000)
        
        model_params["thinking"] = {
            "type": "enabled",
            "budget_tokens": thinking_budget  # ì‚¬ê³  ê³¼ì •ì— í• ë‹¹í•  í† í° ìˆ˜
        }
    
    return bedrock_client, model_params

def convert_langchain_messages_to_anthropic(messages):
    """LangChain ë©”ì‹œì§€ë¥¼ Anthropic API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    result = []
    system_message = None
    
    for msg in messages:
        if msg.type == "system":
            system_message = msg.content  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë³„ë„ ì €ì¥
        elif msg.type == "ai":
            result.append({"role": "assistant", "content": msg.content})
        elif msg.type == "human":
            result.append({"role": "user", "content": msg.content})
    
    return result, system_message

def convert_chat_messages_to_langchain(chat_messages: List[ChatMessage]) -> List[Union[HumanMessage, AIMessage, SystemMessage]]:
    messages = []
    for chat_msg in chat_messages:
        if not chat_msg:
            continue
        if chat_msg.role == "user":
            messages.append(HumanMessage(content=chat_msg.text))
        elif chat_msg.role == "assistant":
            messages.append(AIMessage(content=chat_msg.text))
        elif chat_msg.role == "system":
            messages.append(SystemMessage(content=chat_msg.text))
    return messages

def generate_response(
    conversation_data: Tuple[boto3.client, dict],
    input_text: str,
    chat_history: StreamlitChatMessageHistory,
    show_reasoning: bool = False,
    mcp_enable: bool = False
) -> str:
    # ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì²´í¬
    if len(input_text) > 32000:  # Claude 3ì˜ ìµœëŒ€ ì…ë ¥ í† í° ì œí•œ
        raise ValueError("ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤.")

    # boto3 í´ë¼ì´ì–¸íŠ¸ì™€ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    client, model_params = conversation_data
    model_id = model_params["model_id"]
    system_message = model_params.get("system", "")
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # MCP ì„œë¹„ìŠ¤ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        datetime_info_text = ""
        search_results_text = ""
        
        # MCP í™œì„±í™” ìƒíƒœì—ì„œ ì²˜ë¦¬
        if mcp_enable:
            # ì§ˆì˜ ë¶„ì„ ë° ì„œë¹„ìŠ¤ ì‹¤í–‰
            try:
                # ì´ì „ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                chat_context = None
                if chat_history.messages and len(chat_history.messages) >= 2:
                    # ê°€ì¥ ìµœê·¼ ë©”ì‹œì§€ 2ê°œ(ì‚¬ìš©ì ì§ˆë¬¸-ëª¨ë¸ ì‘ë‹µ)
                    last_messages = chat_history.messages[-2:]
                    chat_context = [msg.content for msg in last_messages]
                
                # 1. Claude 3.7ì„ í™œìš©í•œ ì§ˆì˜ ì˜ë„ ë¶„ì„ (Thinking API)
                st.info("ğŸ§  Claudeë¡œ ì§ˆì˜ ì˜ë„ ë¶„ì„ ì¤‘...")
                
                # ëŒ€í™” ë‚´ìš© ì¶”ì¶œ (ìµœê·¼ ë©”ì‹œì§€ ìµœëŒ€ 4ê°œ)
                chat_context_list = None
                if chat_history and len(chat_history.messages) >= 2:
                    # ê°€ì¥ ìµœê·¼ ë©”ì‹œì§€ ìµœëŒ€ 4ê°œ (2ìŒ)
                    messages = chat_history.messages[-4:] if len(chat_history.messages) >= 4 else chat_history.messages
                    chat_context_list = [msg.content for msg in messages]
                
                # LLMìœ¼ë¡œ ì˜ë„ ë¶„ì„ (ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤)
                intent_analysis = analyze_query_intent_with_llm(input_text, client, chat_context_list)
                
                # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                intent = intent_analysis.get("intent", "general")
                subtype = intent_analysis.get("subtype", "none")
                datetime_needed = intent_analysis.get("datetime_needed", False)
                search_needed = intent_analysis.get("search_needed", True)
                reasoning = intent_analysis.get("reasoning", "")
                
                # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                with st.expander("ğŸ§© ì§ˆì˜ ì˜ë„ ë¶„ì„ ê²°ê³¼", expanded=False):
                    relative_importance = intent_analysis.get("relative_importance", "search")
                    st.markdown(f"""
                    **ì˜ë„ ìœ í˜•:** {intent} {f'({subtype})' if subtype and subtype != 'none' else ''}
                    **ë‚ ì§œ/ì‹œê°„ ì •ë³´ í•„ìš”:** {'âœ…' if datetime_needed else 'âŒ'}
                    **ì›¹ ê²€ìƒ‰ í•„ìš”:** {'âœ…' if search_needed else 'âŒ'} 
                    **ì •ë³´ ìš°ì„ ìˆœìœ„:** {relative_importance.upper()}
                    **ë¶„ì„ ì´ìœ :** {reasoning}
                    """)
                
                # 2. ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ MCP ì„œë¹„ìŠ¤ ì‹¤í–‰
                # ë‚ ì§œ/ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°
                if datetime_needed:
                    try:
                        st.info("â° ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤")
                        dt_info = mcp_client.get_datetime_info()
                        datetime_info_text = mcp_client.format_datetime_info(dt_info)
                        st.success("í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
                        
                        # ê²°ê³¼ í‘œì‹œ
                        with st.expander("ğŸ“… ë‚ ì§œ/ì‹œê°„ ì •ë³´"):
                            st.markdown(datetime_info_text)
                    except Exception as e:
                        st.error(f"ë‚ ì§œ/ì‹œê°„ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                
                # ê²€ìƒ‰ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš°
                if search_needed:
                    # ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
                    keywords = extract_keywords(input_text)
                    search_query = " ".join(keywords)
                    
                    if search_query:
                        st.info(f"ğŸ” Googleì—ì„œ '{search_query}'ì— ëŒ€í•œ ì •ë³´ ê²€ìƒ‰ ì¤‘")
                        search_results = mcp_client.search(search_query)
                        
                        if search_results:
                            search_results_text = mcp_client.format_results(search_results)
                            st.success(f"'{search_query}' ê²€ìƒ‰ ê²°ê³¼ {len(search_results)}ê±´ ë°œê²¬")
                            
                            # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
                            with st.expander("ğŸŒ ê²€ìƒ‰ ê²°ê³¼"):
                                st.markdown(search_results_text)
                        else:
                            st.warning(f"'{search_query}' ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
            except Exception as e:
                st.error(f"MCP ì„œë¹„ìŠ¤ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        try:
            # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            messages = []
            messages.append(SystemMessage(content=system_message))
            for msg in chat_history.messages:
                messages.append(msg)
            
            # MCP ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
            if mcp_enable:
                # ì§ˆì˜ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                relative_importance = intent_analysis.get("relative_importance", "search")
                
                # ê²€ìƒ‰ ê²°ê³¼ì™€ ë‚ ì§œ/ì‹œê°„ ì •ë³´ê°€ ëª¨ë‘ ìˆëŠ” ê²½ìš°
                if search_results_text and datetime_info_text:
                    # ì¤‘ìš”ë„ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    if relative_importance == "search":
                        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë” ì¤‘ìš”í•œ ê²½ìš° (ë‚ ì”¨, ë‰´ìŠ¤ ë“±)
                        enhanced_input = f"""ì§ˆë¬¸: {input_text}
                    
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:

[ì£¼ìš” ì •ë³´] ê²€ìƒ‰ ê²°ê³¼:
{search_results_text}

[ì°¸ê³  ì •ë³´] í˜„ì¬ ë‚ ì§œ/ì‹œê°„:
{datetime_info_text}

ì¤‘ìš” ì§€ì¹¨:
1. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìµœëŒ€í•œ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤"ê³  ë‹µë³€í•˜ê¸° ì „ì— ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.
3. ê²€ìƒ‰ ê²°ê³¼ì— ìˆ«ì, ë°ì´í„°, ì‚¬ì‹¤ ë“±ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê·¸ ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
4. ê²€ìƒ‰ ê²°ê³¼ì˜ ë§í¬ë§Œ ì•ˆë‚´í•˜ì§€ ë§ê³ , ì‹¤ì œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
5. ë‚ ì§œ/ì‹œê°„ ì •ë³´ëŠ” í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë¶€ê°€ ì •ë³´ë¡œ í™œìš©í•˜ì„¸ìš”.
6. ë‹µë³€ì—ì„œ ì¸ìš©í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ì¶œì²˜ë¥¼ [1], [2]ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•´ì£¼ì„¸ìš”."""
                    
                    elif relative_importance == "datetime":
                        # ë‚ ì§œ/ì‹œê°„ì´ ë” ì¤‘ìš”í•œ ê²½ìš° (ì‹œê°„ ê³„ì‚° ë“±)
                        enhanced_input = f"""ì§ˆë¬¸: {input_text}
                    
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”:

[ì£¼ìš” ì •ë³´] í˜„ì¬ ë‚ ì§œ/ì‹œê°„:
{datetime_info_text}

[ì°¸ê³  ì •ë³´] ê²€ìƒ‰ ê²°ê³¼:
{search_results_text}

ì¤‘ìš” ì§€ì¹¨:
1. ë‚ ì§œ/ì‹œê°„ ì •ë³´ë¥¼ ì£¼ìš” ì •ë³´ì›ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.
2. ê²€ìƒ‰ ê²°ê³¼ë„ ì² ì €íˆ ë¶„ì„í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
3. "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤"ê³  ë‹µë³€í•˜ê¸° ì „ì— ì œê³µëœ ëª¨ë“  ì •ë³´ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.
4. ê²€ìƒ‰ ê²°ê³¼ì˜ ë§í¬ë§Œ ì•ˆë‚´í•˜ì§€ ë§ê³ , ì‹¤ì œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
5. ë‹µë³€ì—ì„œ ì¸ìš©í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”."""
                    
                    else:  # "both"
                        # ë‘ ì •ë³´ê°€ ëª¨ë‘ í•„ìš”í•œ ê²½ìš°
                        enhanced_input = f"""ì§ˆë¬¸: {input_text}
                    
ë‹¤ìŒì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ ì¤‘ìš” ì •ë³´ì…ë‹ˆë‹¤:

[1] ê²€ìƒ‰ ê²°ê³¼:
{search_results_text}

[2] í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ì •ë³´:
{datetime_info_text}

ì¤‘ìš” ì§€ì¹¨:
1. ë‘ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
2. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ìµœëŒ€í•œ ì¶”ì¶œí•˜ì„¸ìš”.
3. "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤"ê³  ë‹µë³€í•˜ê¸° ì „ì— ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì² ì €íˆ ë¶„ì„í•˜ì„¸ìš”.
4. ê²€ìƒ‰ ê²°ê³¼ì— ìˆ«ì, ë°ì´í„°, ì‚¬ì‹¤ ë“±ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê·¸ ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
5. ê²€ìƒ‰ ê²°ê³¼ì˜ ë§í¬ë§Œ ì•ˆë‚´í•˜ì§€ ë§ê³ , ì‹¤ì œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
6. ë‹µë³€ì—ì„œ ì¸ìš©í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ì¶œì²˜ë¥¼ [1] ë˜ëŠ” [2]ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•´ì£¼ì„¸ìš”."""
                        
                    messages.append(HumanMessage(content=enhanced_input))
                
                # ê²€ìƒ‰ ê²°ê³¼ë§Œ ìˆëŠ” ê²½ìš°
                elif search_results_text:
                    enhanced_input = f"""ì§ˆë¬¸: {input_text}
                    
ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤:
{search_results_text}

ì¤‘ìš” ì§€ì¹¨:
1. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì² ì €íˆ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ìµœëŒ€í•œ ì¶”ì¶œí•˜ì„¸ìš”.
2. "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤"ê³  ë‹µë³€í•˜ê¸° ì „ì— ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ê²€í† í•˜ì„¸ìš”.
3. ê²€ìƒ‰ ê²°ê³¼ì— ìˆ«ì, ë°ì´í„°, ì‚¬ì‹¤ ë“±ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê·¸ ì •ë³´ë¥¼ ì ê·¹ í™œìš©í•˜ì„¸ìš”.
4. ê²€ìƒ‰ ê²°ê³¼ì˜ ë§í¬ë§Œ ì•ˆë‚´í•˜ì§€ ë§ê³ , ì‹¤ì œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
5. ë‹µë³€ì—ì„œ ì¸ìš©í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ê·¸ ì¶œì²˜(ë²ˆí˜¸)ë¥¼ [1], [2]ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•´ì£¼ì„¸ìš”."""
                    messages.append(HumanMessage(content=enhanced_input))
                
                # ë‚ ì§œ/ì‹œê°„ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš°
                elif datetime_info_text:
                    enhanced_input = f"""ì§ˆë¬¸: {input_text}
                    
ë‹¤ìŒì€ í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ì •ë³´ì…ë‹ˆë‹¤:
{datetime_info_text}

ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ì¶”ê°€ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ê³  ëŠê»´ì§€ë©´ ì†”ì§í•˜ê²Œ ê·¸ ì‚¬ì‹¤ì„ ì•Œë ¤ì£¼ì„¸ìš”."""
                    messages.append(HumanMessage(content=enhanced_input))
                
                # ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°
                else:
                    messages.append(HumanMessage(content=input_text))
            else:
                # MCPê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ì›ë³¸ ì§ˆë¬¸ë§Œ ì „ë‹¬
                messages.append(HumanMessage(content=input_text))
            
            # ë©”ì‹œì§€ë¥¼ Claude API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            anthropic_messages, system_content = convert_langchain_messages_to_anthropic(messages)
            
            # MCPë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° reasoning ëª¨ë“œëŠ” ë¹„í™œì„±í™”
            if mcp_enable and "thinking" in model_params:
                st.warning("MCP ì‚¬ìš© ì‹œ Model reasoning ëª¨ë“œëŠ” ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                has_thinking = False
            else:
                has_thinking = "thinking" in model_params

            # API ìš”ì²­ í˜ì´ë¡œë“œ êµ¬ì„±
            request_payload = {
                "anthropic_version": model_params.get("anthropic_version", "bedrock-2023-05-31"),
                "max_tokens": model_params.get("max_tokens", 8192),
                "temperature": model_params.get("temperature", 0.0),
                "messages": anthropic_messages,
                "system": system_content
            }
            
            # reasoning ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš° thinking íŒŒë¼ë¯¸í„° ì¶”ê°€
            if has_thinking and not mcp_enable:
                request_payload["thinking"] = model_params["thinking"]
                st.info("Model reasoning ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„± ì¤‘...")
            
            # top_pì™€ top_k ê°’ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            if "top_p" in model_params:
                request_payload["top_p"] = model_params["top_p"]
            if "top_k" in model_params:
                request_payload["top_k"] = model_params["top_k"]
            
            try:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                response = client.invoke_model_with_response_stream(
                    modelId=model_id,
                    body=json.dumps(request_payload)
                )
                
                # reasoning_placeholderëŠ” reasoning ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ì‚¬ìš©
                reasoning_placeholder = st.empty() if has_thinking and show_reasoning else None
                reasoning_text = ""
                
                for event in response["body"]:
                    try:
                        chunk = json.loads(event["chunk"]["bytes"])
                        
                        # thinking íƒ€ì… ì²˜ë¦¬ (reasoning ê³¼ì •)
                        if has_thinking and chunk.get("type") == "thinking":
                            if show_reasoning:
                                thinking_content = chunk.get("thinking", "")
                                reasoning_text += thinking_content
                                
                                # reasoning ê³¼ì •ì„ UIì— í‘œì‹œ
                                bg_color = "#2a2a2a"
                                text_color = "#ffffff"
                                
                                reasoning_placeholder.markdown(f"""
                                <div style="background-color: {bg_color}; color: {text_color}; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #007bff; border: 1px solid #007bff; max-height: 400px; overflow-y: auto;">
                                    <h4 style="color: {text_color}; margin-top: 0;">ğŸ§  Reasoning...</h4>
                                    <pre style="white-space: pre-wrap; overflow-wrap: break-word; color: {text_color}; background-color: transparent; border: none; padding: 0; margin: 0;">{reasoning_text}</pre>
                                </div>
                                """, unsafe_allow_html=True)
                        
                        # content_block_delta íƒ€ì… ì²˜ë¦¬
                        elif chunk.get("type") == "content_block_delta":
                            if chunk["delta"].get("type") == "text" or chunk["delta"].get("type") == "text_delta":
                                text_chunk = chunk["delta"].get("text", "")
                                full_response += text_chunk
                                
                                # ì¼ì • ê°„ê²©ìœ¼ë¡œ UI ì—…ë°ì´íŠ¸
                                if len(text_chunk) > 10 or text_chunk.endswith(('.', '!', '?', '\n')):
                                    message_placeholder.markdown(full_response + "â–Œ")
                                    
                            # thinking_delta ì²˜ë¦¬
                            elif has_thinking and show_reasoning and chunk["delta"].get("type") == "thinking_delta":
                                thinking_chunk = chunk["delta"].get("thinking", "")
                                if thinking_chunk:
                                    reasoning_text += thinking_chunk
                                    
                                    # UI ì—…ë°ì´íŠ¸
                                    bg_color = "#2a2a2a" 
                                    text_color = "#ffffff"
                                    
                                    reasoning_placeholder.markdown(f"""
                                    <div style="background-color: {bg_color}; color: {text_color}; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #007bff; border: 1px solid #007bff; max-height: 400px; overflow-y: auto;">
                                        <h4 style="color: {text_color}; margin-top: 0;">ğŸ§  Reasoning...</h4>
                                        <pre style="white-space: pre-wrap; overflow-wrap: break-word; color: {text_color}; background-color: transparent; border: none; padding: 0; margin: 0;">{reasoning_text}</pre>
                                    </div>
                                    """, unsafe_allow_html=True)
                        
                        # ì‘ë‹µ ì™„ë£Œ ì²˜ë¦¬
                        elif chunk.get("type") == "message_stop":
                            message_placeholder.markdown(full_response)
                    except Exception as chunk_error:
                        print(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(chunk_error)}")
                
                message_placeholder.markdown(full_response)
                return full_response
                
            except Exception as e:
                st.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í˜¸ì¶œ ì‹œë„
                try:
                    response = client.invoke_model(
                        modelId=model_id,
                        body=json.dumps(request_payload)
                    )
                    response_body = json.loads(response["body"].read().decode("utf-8"))
                    full_response = response_body.get("completion", "") or response_body.get("content", "")
                    message_placeholder.markdown(full_response)
                    return full_response
                except Exception as e2:
                    st.error(f"ì¼ë°˜ í˜¸ì¶œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e2)}")
                    return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e2)}"
            
        except Exception as e:
            error_detail = str(e)
            print(f"ìƒì„¸ ì˜¤ë¥˜: {error_detail}")  # í„°ë¯¸ë„ì— ìì„¸í•œ ì˜¤ë¥˜ ì¶œë ¥
            
            if "ThrottlingException" in error_detail:
                error_message = "ìš”ì²­ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ™"
            elif "validationException" in error_detail:
                error_message = "API ê²€ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìš”ì²­ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                print(f"API ê²€ì¦ ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
            else:
                error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_detail}"
            
            message_placeholder.markdown(error_message)
            return error_message

def analyze_query_intent_with_llm(query: str, client, chat_history=None) -> Dict:
    """
    Claude 3.7 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆì˜ ì˜ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        query: ì‚¬ìš©ì ì§ˆì˜
        client: boto3 bedrock-runtime í´ë¼ì´ì–¸íŠ¸
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì )
        
    Returns:
        Dict: {
            "intent": ì˜ë„ (datetime, search, mixed, general ì¤‘ í•˜ë‚˜),
            "subtype": ì„¸ë¶€ ìœ í˜• (datetimeì¸ ê²½ìš° time, date ë“±),
            "datetime_needed": ë‚ ì§œ/ì‹œê°„ ì •ë³´ í•„ìš” ì—¬ë¶€ (bool),
            "search_needed": ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ (bool),
            "relative_importance": ì •ë³´ì˜ ìƒëŒ€ì  ì¤‘ìš”ë„ (search, datetime, both ì¤‘ í•˜ë‚˜),
            "reasoning": ë¶„ì„ ì´ìœ 
        }
    """
    # ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = ""
    if chat_history and len(chat_history) > 0:
        last_messages = chat_history[-4:] if len(chat_history) > 4 else chat_history
        context = "ì´ì „ ëŒ€í™”:\n" + "\n".join([f"{'ì‚¬ìš©ì' if i % 2 == 0 else 'ì–´ì‹œìŠ¤í„´íŠ¸'}: {msg}" for i, msg in enumerate(last_messages)])
    
    # ì˜ë„ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆì˜ì˜ ì˜ë„ë¥¼ ì •í™•í•˜ê²Œ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ ìœ í˜•ê³¼ ì„œë¹„ìŠ¤ë¥¼ íŒë‹¨í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ë‘ ê°€ì§€ ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. ë‚ ì§œ/ì‹œê°„ ì •ë³´: í˜„ì¬ ì‹œê°„, ë‚ ì§œ, ìš”ì¼ ë“±ì˜ ì •ë³´
2. ì›¹ ê²€ìƒ‰: ì¸í„°ë„·ì—ì„œ íŠ¹ì • ì •ë³´ë¥¼ ê²€ìƒ‰

ë¶„ì„ ì‹œ ì£¼ì˜ì‚¬í•­:
- "ì˜¤ëŠ˜", "ì§€ê¸ˆ", "í˜„ì¬" ê°™ì€ ì‹œê°„ í‘œí˜„ì´ ìˆë”ë¼ë„, ì‹¤ì œë¡œ í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ì„¸ìš”.
- ì‹œê°„ í‘œí˜„ì„ í¬í•¨í•œ ì§ˆë¬¸ì´ë”ë¼ë„, ê²€ìƒ‰ ê²°ê³¼ë§Œìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
- ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ë§Œ í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤:
  * ëª…ì‹œì ì¸ ì‹œê°„/ë‚ ì§œ ì§ˆë¬¸ ("ì§€ê¸ˆ ëª‡ ì‹œì•¼?", "ì˜¤ëŠ˜ ë¬´ìŠ¨ ìš”ì¼ì´ì•¼?")
  * ë‘ ì‹œê°„ ì‚¬ì´ì˜ ê³„ì‚°ì´ í•„ìš”í•œ ê²½ìš° ("ì¶œì‹œëœì§€ ì–¼ë§ˆë‚˜ ëì–´?")
  * ë‚ ì§œì— ì˜ì¡´ì ì¸ ì •ë³´ ê³„ì‚° ("ì˜¤ëŠ˜ ìŒë ¥ìœ¼ë¡œ ë©°ì¹ ì´ì•¼?")

íŠ¹íˆ ë‹¤ìŒ ìœ í˜•ì˜ ì§ˆë¬¸ì€ ì‹œê°„ í‘œí˜„ì€ ìˆì§€ë§Œ ì‹¤ì œ ë‚ ì§œ/ì‹œê°„ APIê°€ ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?" â†’ ê²€ìƒ‰ì„ í†µí•´ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
- "ì§€ê¸ˆ ì¸ê¸° ì˜í™”ëŠ”?" â†’ ê²€ìƒ‰ì„ í†µí•´ ìµœì‹  ì¸ê¸° ì˜í™” ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
- "ì˜¤ëŠ˜ ì£¼ìš” ë‰´ìŠ¤ëŠ”?" â†’ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
- "í˜„ì¬ ì£¼ì‹ ì‹œì¥ì€?" â†’ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ì£¼ì‹ ì‹œì¥ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ

JSON í˜•ì‹ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”:
{
  "intent": "datetime|search|mixed|general", 
  "subtype": "time|date|datetime|none",
  "datetime_needed": true/false,
  "search_needed": true/false,
  "relative_importance": "search|datetime|both",
  "reasoning": "ë¶„ì„ì— ëŒ€í•œ ì„¤ëª…"
}

relative_importanceëŠ” ë‹¤ìŒê³¼ ê°™ì´ íŒë‹¨í•˜ì„¸ìš”:
- "search": ê²€ìƒ‰ ê²°ê³¼ê°€ ì£¼ìš” ì •ë³´ì›ì¸ ê²½ìš° (ë‚ ì”¨, ë‰´ìŠ¤, ì œí’ˆ ì •ë³´ ë“±)
- "datetime": í˜„ì¬ ë‚ ì§œ/ì‹œê°„ì´ ì£¼ìš” ì •ë³´ì›ì¸ ê²½ìš° (ì‹œê°„ ê³„ì‚°, ìš”ì¼ í™•ì¸ ë“±)
- "both": ë‘ ì •ë³´ê°€ ëª¨ë‘ ì¤‘ìš”í•œ ê²½ìš° (íŠ¹ì • ë‚ ì§œë¡œë¶€í„° ê²½ê³¼ ì‹œê°„ ë“±)
"""

    # ì§ˆì˜ í…ìŠ¤íŠ¸
    query_text = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

{context}

ì´ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì„œë¹„ìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."""

    try:
        # Claude 3.7ì— ë©”ì‹œì§€ ì „ì†¡
        response = client.invoke_model(
            modelId="us.anthropic.claude-3-7-sonnet-20250219-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "temperature": 0,
                "system": system_prompt,
                "messages": [
                    {"role": "user", "content": query_text}
                ]
            })
        )
        
        # ì‘ë‹µ íŒŒì‹±
        response_body = json.loads(response['body'].read())
        response_content = response_body.get('content', [{'text': '{"intent": "general", "datetime_needed": false, "search_needed": false, "reasoning": "ë¶„ì„ ì‹¤íŒ¨"}'}])[0]['text']
        
        # JSON ì¶”ì¶œ (ì‘ë‹µ í…ìŠ¤íŠ¸ì— JSON ì´ì™¸ì˜ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
        json_start = response_content.find('{')
        json_end = response_content.rfind('}') + 1
        if json_start >= 0 and json_end > json_start:
            json_str = response_content[json_start:json_end]
            try:
                result = json.loads(json_str)
                
                # í•„ìˆ˜ í‚¤ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì¶”ê°€
                if "intent" not in result:
                    result["intent"] = "general"
                if "datetime_needed" not in result:
                    result["datetime_needed"] = False
                if "search_needed" not in result:
                    result["search_needed"] = False
                if "subtype" not in result:
                    result["subtype"] = "none"
                if "relative_importance" not in result:
                    # ê¸°ë³¸ê°’ ì„¤ì • - datetime_neededê°€ trueì´ë©´ datetime, ì•„ë‹ˆë©´ search
                    result["relative_importance"] = "datetime" if result.get("datetime_needed", False) and not result.get("search_needed", True) else "search"
                
                return result
            except json.JSONDecodeError:
                print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {json_str}")
                # ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    "intent": "general",
                    "subtype": "none",
                    "datetime_needed": False,
                    "search_needed": True,
                    "reasoning": "ì˜ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
                }
        else:
            print(f"JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {response_content}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "intent": "general", 
                "subtype": "none",
                "datetime_needed": False, 
                "search_needed": True,
                "reasoning": "ì˜ë„ ë¶„ì„ ì‹¤íŒ¨"
            }
            
    except Exception as e:
        print(f"ì˜ë„ ë¶„ì„ ìš”ì²­ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "intent": "general",
            "subtype": "none", 
            "datetime_needed": False,
            "search_needed": True,
            "reasoning": f"ì˜¤ë¥˜: {str(e)}"
        }

def analyze_query_intent(query: str, chat_history=None) -> tuple[str, str]:
    """
    ì‚¬ìš©ì ì§ˆì˜ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ì„œë¹„ìŠ¤ ìœ í˜•ê³¼ ì„¸ë¶€ ìœ í˜•ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ê·œì¹™ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­ ë°©ì‹ (ë ˆê±°ì‹œ ë²„ì „)
    
    Args:
        query: ì‚¬ìš©ì ì§ˆì˜
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ì„ íƒì )
        
    Returns:
        tuple: (ì˜ë„ ìœ í˜•, ì„¸ë¶€ ìœ í˜•)
            ì˜ë„ ìœ í˜•: "datetime", "search", "mixed", "general" ì¤‘ í•˜ë‚˜
            ì„¸ë¶€ ìœ í˜•: datetimeì¸ ê²½ìš° "time", "date", "datetime" ì¤‘ í•˜ë‚˜
    """
    query_lower = query.lower()
    
    # ê° íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼ ì €ì¥
    pattern_matches = {
        "time": any(re.search(pattern, query_lower) for pattern in TIME_PATTERNS),
        "date": any(re.search(pattern, query_lower) for pattern in DATE_PATTERNS),
        "datetime": any(re.search(pattern, query_lower) for pattern in DATETIME_PATTERNS),
        "current_time": any(re.search(pattern, query_lower) for pattern in CURRENT_TIME_PATTERNS),
        "time_comparison": any(re.search(pattern, query_lower) for pattern in TIME_COMPARISON_PATTERNS),
        "search": any(re.search(pattern, query_lower) for pattern in SEARCH_PATTERNS),
        "ambiguous": any(re.search(pattern, query_lower) for pattern in AMBIGUOUS_TERMS)
    }
    
    # 1. ì‹œê°„ ë¹„êµ íŒ¨í„´ ê°ì§€ (ë‚ ì§œ ê°„ ê³„ì‚° ë“±)
    if pattern_matches["time_comparison"]:
        # ì‹œê°„ ë¹„êµ + í˜„ì¬ ì‹œê°„ ì–¸ê¸‰ + ê²€ìƒ‰/ì„ì˜ì–´ = ë³µí•© ì˜ë„
        if (pattern_matches["current_time"] or pattern_matches["date"] or pattern_matches["time"]) and \
           (pattern_matches["search"] or pattern_matches["ambiguous"]):
            return QueryIntent.MIXED, "datetime"
        # ì‹œê°„ ë¹„êµ + í˜„ì¬ ì‹œê°„ ì–¸ê¸‰ = ë‚ ì§œ ì‹œê°„ ì˜ë„
        elif pattern_matches["current_time"] or pattern_matches["date"] or pattern_matches["time"]:
            return QueryIntent.DATETIME, "datetime"
    
    # 2. í˜¼ë™ ê°€ëŠ¥ì–´ê°€ ìˆì§€ë§Œ, ì‹œê°„ ë¹„êµ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° = ë³µí•© ì˜ë„
    if pattern_matches["ambiguous"] and (pattern_matches["time_comparison"] or (pattern_matches["current_time"] and pattern_matches["search"])):
        return QueryIntent.MIXED, "datetime"
    
    # 3. ë‚ ì§œ/ì‹œê°„ íŒ¨í„´ê³¼ ê²€ìƒ‰ íŒ¨í„´ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš° = ë³µí•© ì˜ë„
    if (pattern_matches["time"] or pattern_matches["date"] or pattern_matches["datetime"]) and \
       (pattern_matches["search"] or pattern_matches["ambiguous"]):
        return QueryIntent.MIXED, "datetime"
    
    # 4. ë‚ ì§œ/ì‹œê°„ íŒ¨í„´ë§Œ ìˆëŠ” ê²½ìš°
    if pattern_matches["time"] or pattern_matches["date"] or pattern_matches["datetime"]:
        # ì‹œê°„ê³¼ ë‚ ì§œ íŒ¨í„´ì´ ëª¨ë‘ ìˆìœ¼ë©´ datetime
        if pattern_matches["time"] and (pattern_matches["date"] or pattern_matches["datetime"]):
            return QueryIntent.DATETIME, "datetime"
        # ì‹œê°„ íŒ¨í„´ë§Œ ìˆìœ¼ë©´ time
        elif pattern_matches["time"]:
            return QueryIntent.DATETIME, "time"
        # ë‚ ì§œ íŒ¨í„´ë§Œ ìˆìœ¼ë©´ date
        else:
            return QueryIntent.DATETIME, "date"
    
    # 5. ê²€ìƒ‰ ê´€ë ¨ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° = ê²€ìƒ‰ ì˜ë„
    if pattern_matches["search"] or pattern_matches["ambiguous"]:
        return QueryIntent.SEARCH, ""
    
    # 6. íŠ¹ì • íŒ¨í„´ì´ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì¼ë°˜ ì˜ë„ë¡œ ê°„ì£¼
    return QueryIntent.GENERAL, ""

def process_mcp_services(input_text: str) -> tuple[str, str]:
    """
    ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ MCP ì„œë¹„ìŠ¤ë¥¼ ê²°ì •í•˜ê³  ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        input_text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        
    Returns:
        tuple: (ë‚ ì§œ/ì‹œê°„ ì •ë³´, ê²€ìƒ‰ ê²°ê³¼)
    """
    # ì‚¬ìš©ì ì…ë ¥ ë¶„ì„
    intent, subtype = analyze_query_intent(input_text)
    datetime_info_text = ""
    search_results_text = ""
    
    st.info(f"MCP ì„œë¹„ìŠ¤ ë¶„ì„ ì¤‘: {intent} ì˜ë„ ê°ì§€ë¨")
    
    # ì˜ë„ì— ë”°ë¥¸ ì„œë¹„ìŠ¤ í˜¸ì¶œ
    if intent == QueryIntent.DATETIME or intent == QueryIntent.MIXED:
        try:
            # í†µí•© MCP í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
            if subtype == "time":
                # ì‹œê°„ ì •ë³´ ìš”ì²­
                time_info = mcp_client.get_current_time()
                datetime_info_text = mcp_client.format_time(time_info)
                st.success("ì‹œê°„ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
            elif subtype == "date":
                # ë‚ ì§œ ì •ë³´ ìš”ì²­
                date_info = mcp_client.get_current_date()
                datetime_info_text = mcp_client.format_date(date_info)
                st.success("ë‚ ì§œ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
            else:  # "datetime"
                # ì¢…í•© ì •ë³´ ìš”ì²­
                dt_info = mcp_client.get_datetime_info()
                datetime_info_text = mcp_client.format_datetime_info(dt_info)
                st.success("ë‚ ì§œ ë° ì‹œê°„ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
            
            # ê²°ê³¼ í‘œì‹œ
            with st.expander("ë‚ ì§œ/ì‹œê°„ ì •ë³´ í™•ì¸"):
                st.markdown(datetime_info_text)
                
        except Exception as e:
            st.error(f"ë‚ ì§œ/ì‹œê°„ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    if intent == QueryIntent.SEARCH or intent == QueryIntent.MIXED:
        try:
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = extract_keywords(input_text)
            search_query = " ".join(keywords)
            
            if search_query:
                st.info(f"Googleì—ì„œ '{search_query}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ê²€ìƒ‰ ì¤‘ì…ë‹ˆë‹¤...")
                
                # í†µí•© MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•œ ê²€ìƒ‰ ìˆ˜í–‰
                search_results = mcp_client.search(search_query)
                
                # ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ…
                search_results_text = mcp_client.format_results(search_results)
                
                if search_results:
                    st.success(f"'{search_query}' ê´€ë ¨ ê²€ìƒ‰ ì™„ë£Œ")
                    
                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê¸° (ì ‘ì„ ìˆ˜ ìˆê²Œ)
                    with st.expander("ê²€ìƒ‰ ê²°ê³¼ í™•ì¸"):
                        st.markdown(search_results_text)
                else:
                    st.warning(f"'{search_query}' ê´€ë ¨ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    return datetime_info_text, search_results_text

def new_chat() -> None:
    """ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"""
    # widget_keyë¥¼ ìƒˆë¡œ ìƒì„±í•˜ì—¬ file_uploaderë¥¼ í¬í•¨í•œ ëª¨ë“  ìœ„ì ¯ ì´ˆê¸°í™”
    st.session_state["widget_key"] = str(random.randint(1, 1000000))
    st.session_state.messages = []
    st.session_state.chat_history.clear()
    st.session_state.context = None
    st.session_state.initial_system_message = None

def handle_file_upload(uploaded_file) -> str:
    """íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        try:
            document_text = process_uploaded_file(tmp_file_path)
            st.session_state.context = document_text  # ëŒ€ìš©ëŸ‰ ë¬¸ì„œì˜ ê²½ìš° ë©”ëª¨ë¦¬ ë¶€ì¡± ë°œìƒ ê°€ëŠ¥
            os.unlink(tmp_file_path)
            return document_text
        except Exception as e:
            os.unlink(tmp_file_path)  # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì„ì‹œ íŒŒì¼ ì‚­ì œ í•„ìš”
            st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
    return None

def main() -> None:
    set_page_config()
    initialize_session_state()

    st.sidebar.button("New Chat", on_click=new_chat, type="primary")

    temperature, top_p, top_k, max_tokens, memory_window, system_prompt, uploaded_file, model_name, extended_thinking, show_reasoning, mcp_enable = get_sidebar_params()

    # ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì´ˆê¸°í™”
    if uploaded_file:
        document_context = handle_file_upload(uploaded_file)
        if document_context:
            st.sidebar.success(f"ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {uploaded_file.name}")
            # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±
            full_system_prompt = f"{system_prompt}\n\nì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©:\n\n{document_context}"
            st.session_state.initial_system_message = full_system_prompt

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if st.session_state.initial_system_message:
        system_prompt = st.session_state.initial_system_message

    conv_chain = init_conversation_chain(
        temperature, top_p, top_k, max_tokens, system_prompt, model_name, extended_thinking
    )

    # ì €ì¥ëœ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.session_state.chat_history.add_user_message(prompt)
        
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì‘ë‹µ ìƒì„± ë° ì„¸ì…˜ ì €ì¥ (UI í‘œì‹œëŠ” generate_responseì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
        response = generate_response(conv_chain, prompt, st.session_state.chat_history, show_reasoning, mcp_enable)
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ë§Œ ìˆ˜í–‰ (UI í‘œì‹œëŠ” í•˜ì§€ ì•ŠìŒ)
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.session_state.chat_history.add_ai_message(response)

if __name__ == "__main__":
    main()

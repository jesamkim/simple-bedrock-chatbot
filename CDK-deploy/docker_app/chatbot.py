import random
from typing import List, Tuple, Union
import os
import tempfile
import csv
import pandas as pd

import streamlit as st
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_aws import ChatBedrock, ChatBedrockConverse
from langchain_core.messages import AIMessage, HumanMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.runnables import RunnableWithMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import boto3

REGION = "us-west-2"
MODELS = {
    "Claude 3.5 Sonnet v2": {
        "id": "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        "class": ChatBedrock,
        "use_model_kwargs": True,
        "max_tokens": 4096
    },
    "Nova Pro 1.0": {
        "id": "us.amazon.nova-pro-v1:0",
        "class": None,  # ì§ì ‘ boto3 client ì‚¬ìš©
        "use_model_kwargs": False,
        "max_tokens": 5000
    }
}

SUPPORTED_FORMATS = ['pdf', 'doc', 'docx', 'md', 'ppt', 'pptx', 'txt', 'html', 'csv', 'xls', 'xlsx']

CLAUDE_PROMPT = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="history"),
    MessagesPlaceholder(variable_name="input"),
])

def get_init_message(model_name: str) -> dict:
    return {
        "role": "assistant",
        "content": f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI ë„ìš°ë¯¸ ì…ë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì‹œë©´ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
    }

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container: st.container) -> None:
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

def set_page_config() -> None:
    st.set_page_config(page_title="Bedrock Chatbot", layout="wide")
    st.title("Bedrock Chatbot with Document Q&A")

def get_sidebar_params() -> Tuple[float, float, int, int, int, str]:
    with st.sidebar:
        st.markdown("## Model Selection")
        model_name = st.radio(
            "Choose a model",
            options=list(MODELS.keys()),
            index=0,
            key=f"{st.session_state['widget_key']}_Model"
        )
        
        st.markdown("## Document Upload")
        uploaded_file = st.file_uploader(
            "Upload a document",
            type=SUPPORTED_FORMATS,
            help="ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: pdf, doc, docx, md, ppt, pptx, txt, html, csv, xls, xlsx"
        )

        st.markdown("## Inference Parameters")
        system_prompt = st.text_area(
            "System Prompt",
            "You're a helpful assistant who loves to respond in Korean. You'll answer questions based on the uploaded documents. When you're not certain about something, don't guess or make up information. Instead, honestly admit that you don't know.",
            key=f"{st.session_state['widget_key']}_System_Prompt",
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.1,
            key=f"{st.session_state['widget_key']}_Temperature",
        )

        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                top_p = st.slider(
                    "Top-P",
                    min_value=0.0,
                    max_value=1.0,
                    value=1.00,
                    step=0.01,
                    key=f"{st.session_state['widget_key']}_Top-P",
                )
            with col2:
                top_k = st.slider(
                    "Top-K",
                    min_value=1,
                    max_value=500,
                    value=500,
                    step=5,
                    key=f"{st.session_state['widget_key']}_Top-K",
                )

        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                # ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ max_tokens ìŠ¬ë¼ì´ë” ë²”ìœ„ ì¡°ì •
                model_max_tokens = MODELS[model_name]["max_tokens"]
                max_tokens = st.slider(
                    "Max Token",
                    min_value=0,
                    max_value=model_max_tokens,
                    value=model_max_tokens,
                    step=8,
                    key=f"{st.session_state['widget_key']}_Max_Token",
                )
            with col2:
                memory_window = st.slider(
                    "Memory Window",
                    min_value=0,
                    max_value=10,
                    value=10,
                    step=1,
                    key=f"{st.session_state['widget_key']}_Memory_Window",
                )

    return temperature, top_p, top_k, max_tokens, memory_window, system_prompt, uploaded_file, model_name

def process_text_based_file(file_path: str) -> str:
    """í…ìŠ¤íŠ¸ ê¸°ë°˜ íŒŒì¼(txt, md, html) ì²˜ë¦¬"""
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

def process_excel_file(file_path: str) -> str:
    """ì—‘ì…€ íŒŒì¼(xls, xlsx) ì²˜ë¦¬"""
    df = pd.read_excel(file_path)
    return df.to_string()

def process_csv_file(file_path: str) -> str:
    """CSV íŒŒì¼ ì²˜ë¦¬"""
    df = pd.read_csv(file_path)
    return df.to_string()

def process_powerpoint_file(file_path: str) -> str:
    """íŒŒì›Œí¬ì¸íŠ¸ íŒŒì¼(ppt, pptx) ì²˜ë¦¬"""
    from pptx import Presentation
    
    text = []
    try:
        prs = Presentation(file_path)
        for slide in prs.slides:
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text.append(shape.text)
    except Exception as e:
        st.warning(f"ì¼ë¶€ ìŠ¬ë¼ì´ë“œë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    return "\n\n".join(text)

def process_uploaded_file(file_path: str) -> str:
    """ë¬¸ì„œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_ext == '.pdf':
            from PyPDF2 import PdfReader
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text
        
        elif file_ext in ['.doc', '.docx']:
            from docx import Document
            doc = Document(file_path)
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        
        elif file_ext in ['.txt', '.md', '.html']:
            return process_text_based_file(file_path)
        
        elif file_ext in ['.csv']:
            return process_csv_file(file_path)
        
        elif file_ext in ['.xls', '.xlsx']:
            return process_excel_file(file_path)
        
        elif file_ext in ['.ppt', '.pptx']:
            return process_powerpoint_file(file_path)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_ext}")
            
    except Exception as e:
        raise Exception(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({file_ext}): {str(e)}")

def init_conversationchain(
    temperature: float,
    top_p: float,
    top_k: int,
    max_tokens: int,
    memory_window: int,
    system_prompt: str,
    model_name: str,
    context: str = None
) -> Union[RunnableWithMessageHistory, boto3.client]:

    model_info = MODELS[model_name]
    model_id = model_info["id"]
    use_model_kwargs = model_info["use_model_kwargs"]

    # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
    if context:
        full_system_prompt = f"{system_prompt}\n\nì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©:\n\n{context}"
    else:
        full_system_prompt = system_prompt

    if use_model_kwargs:  # ChatBedrock (Claude 3.5 Sonnet)
        model_kwargs = {
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "max_tokens": max_tokens,
            "system": full_system_prompt
        }
        llm = ChatBedrock(
            model_id=model_id,
            model_kwargs=model_kwargs,
            streaming=True,
            region_name=REGION
        )

        prompt = ChatPromptTemplate.from_messages([
            ("system", full_system_prompt),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])

        chain = prompt | llm

        def get_session_history():
            return StreamlitChatMessageHistory()

        conversation = RunnableWithMessageHistory(
            runnable=chain,
            get_session_history=get_session_history,
            input_messages_key="input",
            history_messages_key="history"
        )

        return conversation
    else:  # Nova Pro - ì§ì ‘ boto3 client ì‚¬ìš©
        return boto3.client("bedrock-runtime", region_name=REGION)

def generate_response(
    conversation: Union[RunnableWithMessageHistory, boto3.client],
    input: Union[str, List[dict]]
) -> str:
    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        try:
            if isinstance(conversation, RunnableWithMessageHistory):  # Claude 3.5 Sonnet
                response = conversation.invoke(
                    {"input": input},
                    {"callbacks": [stream_handler]}
                )
                if isinstance(response, str):
                    stream_handler.container.markdown(response)
                    return response
                return stream_handler.text
            else:  # Nova Pro
                # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ user ë©”ì‹œì§€ì— í¬í•¨
                system_message = st.session_state[f"{st.session_state['widget_key']}_System_Prompt"]
                if "document_context" in st.session_state:
                    system_message = f"{system_message}\n\nì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©:\n\n{st.session_state['document_context']}"
                
                messages = [
                    {"role": "user", "content": [{"text": system_message}]},
                    {"role": "assistant", "content": [{"text": "ì•Œê² ìŠµë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ê² ìŠµë‹ˆë‹¤."}]},
                    {"role": "user", "content": [{"text": input[0]["content"][0]["text"]}]}
                ]
                
                response = conversation.converse_stream(
                    modelId=MODELS["Nova Pro 1.0"]["id"],
                    messages=messages,
                    inferenceConfig={
                        "temperature": float(st.session_state[f"{st.session_state['widget_key']}_Temperature"]),
                        "maxTokens": int(st.session_state[f"{st.session_state['widget_key']}_Max_Token"]),
                    }
                )

                full_response = ""
                for event in response['stream']:
                    if 'contentBlockDelta' in event:
                        delta_text = event['contentBlockDelta']['delta']['text']
                        full_response += delta_text
                        stream_handler.container.markdown(full_response)
                
                return full_response

        except Exception as e:
            if "ThrottlingException" in str(e):
                error_message = "ìš”ì²­ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ™"
            else:
                error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            stream_handler.container.markdown(error_message)
            return error_message

def new_chat() -> None:
    st.session_state["messages"] = [get_init_message(st.session_state[f"{st.session_state['widget_key']}_Model"])]
    st.session_state["langchain_messages"] = []
    if "document_context" in st.session_state:
        del st.session_state["document_context"]

def display_chat_messages() -> None:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "user":
                if isinstance(message["content"], str):
                    st.markdown(message["content"])
                elif isinstance(message["content"], dict):
                    st.markdown(message["content"]["input"][0]["content"][0]["text"])
                else:
                    st.markdown(message["content"][0]["text"])

            if message["role"] == "assistant":
                if isinstance(message["content"], str):
                    st.markdown(message["content"])
                elif "response" in message["content"]:
                    st.markdown(message["content"]["response"])

def langchain_messages_format(messages: List[Union[AIMessage, HumanMessage, dict]]) -> List[Union[AIMessage, HumanMessage]]:
    formatted_messages = []
    for message in messages:
        if isinstance(message, (AIMessage, HumanMessage)):
            if isinstance(message.content, list):
                if "role" in message.content[0]:
                    if message.type == "ai":
                        message = AIMessage(content=message.content[0]["content"])
                    if message.type == "human":
                        message = HumanMessage(content=message.content[0]["content"])
            formatted_messages.append(message)
        elif isinstance(message, dict):
            if message["role"] == "ai" or message["role"] == "assistant":
                formatted_messages.append(AIMessage(content=message["content"]))
            elif message["role"] == "human" or message["role"] == "user":
                formatted_messages.append(HumanMessage(content=message["content"]))
    return formatted_messages

def handle_file_upload(uploaded_file) -> str:
    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        try:
            # ë¬¸ì„œ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            document_text = process_uploaded_file(tmp_file_path)
            st.session_state["document_context"] = document_text
            os.unlink(tmp_file_path)
            return document_text
        except Exception as e:
            os.unlink(tmp_file_path)
            st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
    return None

def main() -> None:
    set_page_config()

    if "widget_key" not in st.session_state:
        st.session_state["widget_key"] = str(random.randint(1, 1000000))
    if "messages" not in st.session_state:
        st.session_state.messages = [get_init_message("Claude 3.5 Sonnet v2")]
    if "langchain_messages" not in st.session_state:
        st.session_state.langchain_messages = []

    st.sidebar.button("New Chat", on_click=new_chat, type="primary")

    temperature, top_p, top_k, max_tokens, memory_window, system_prompt, uploaded_file, model_name = get_sidebar_params()

    document_context = None
    if uploaded_file:
        document_context = handle_file_upload(uploaded_file)
        if document_context:
            st.sidebar.success(f"ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {uploaded_file.name}")
    elif "document_context" in st.session_state:
        document_context = st.session_state["document_context"]

    conv_chain = init_conversationchain(
        temperature, top_p, top_k, max_tokens, memory_window, system_prompt, model_name, document_context
    )

    display_chat_messages()

    prompt = st.chat_input()

    if prompt:
        prompt_text = {"type": "text", "text": prompt}
        prompt_new = [prompt_text]
        st.session_state.messages.append({"role": "user", "content": prompt_new})
        with st.chat_message("user"):
            st.markdown(prompt)

    if st.session_state.langchain_messages:
        st.session_state.langchain_messages = langchain_messages_format(
            st.session_state.langchain_messages
        )

    if st.session_state.messages[-1]["role"] != "assistant":
        response = generate_response(
            conv_chain, [{"role": "user", "content": prompt_new}]
        )

        message = {"role": "assistant", "content": response}
        st.session_state.messages.append(message)

if __name__ == "__main__":
    main()

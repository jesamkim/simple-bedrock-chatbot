import random
import json
from typing import List, Tuple, Union
import os
import tempfile
import csv
import pandas as pd
import warnings

# Pydantic ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="pydantic")

import streamlit as st
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_aws import ChatBedrock, ChatBedrockConverse
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
import boto3

REGION = "us-west-2"
MODELS = {
    "Claude 3.7 Sonnet": {
        "id": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "class": ChatBedrock,
        "use_model_kwargs": True,
        "max_tokens": 8192
    },
    "Nova Pro 1.0": {
        "id": "us.amazon.nova-pro-v1:0",
        "class": None,  # ì§ì ‘ boto3 client ì‚¬ìš©
        "use_model_kwargs": False,
        "max_tokens": 5000
    }
}

SUPPORTED_FORMATS = ['pdf', 'doc', 'docx', 'md', 'ppt', 'pptx', 'txt', 'html', 'csv', 'xls', 'xlsx']
MAX_MESSAGES = 10  # ëŒ€í™” ê¸°ë¡ ìµœëŒ€ ìœ ì§€ ìˆ˜

class ChatMessage:
    def __init__(self, role: str, text: str):
        self.role = role
        self.text = text

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "widget_key" not in st.session_state:
        st.session_state["widget_key"] = str(random.randint(1, 1000000))
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = StreamlitChatMessageHistory(key="chat_history")
    if "context" not in st.session_state:
        st.session_state.context = None
    if "nova_messages" not in st.session_state:
        st.session_state.nova_messages = []
    if "initial_system_message" not in st.session_state:
        st.session_state.initial_system_message = None

class StreamHandler(BaseCallbackHandler):
    def __init__(self, container: st.container) -> None:
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

def set_page_config() -> None:
    st.set_page_config(page_title="Bedrock Chatbot", layout="wide")
    st.title("Bedrock Chatbot with Document Q&A")

def get_sidebar_params() -> Tuple[float, float, int, int, int, str, object, str, bool, bool]:
    with st.sidebar:
        st.markdown("## Model Selection")
        model_name = st.radio(
            "Choose a model",
            options=list(MODELS.keys()),
            index=0,
            key=f"{st.session_state['widget_key']}_Model"
        )
        
        # Claude 3.7 Sonnet ëª¨ë¸ì´ ì„ íƒëœ ê²½ìš°ì—ë§Œ Model reasoning ëª¨ë“œ ì˜µì…˜ í‘œì‹œ
        extended_thinking = False  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹„í™œì„±í™”
        show_reasoning = False  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹„í™œì„±í™”
        if model_name == "Claude 3.7 Sonnet":
            extended_thinking = st.checkbox(
                "Model reasoning ëª¨ë“œ í™œì„±í™”",
                value=False,  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë¹„í™œì„±í™”
                help="Claude 3.7 Sonnetì˜ ì¶”ë¡  ëª¨ë“œë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤. ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì— ë„ì›€ì´ ë©ë‹ˆë‹¤. (ì°¸ê³ : ì´ ëª¨ë“œì—ì„œëŠ” Temperatureê°€ ìë™ìœ¼ë¡œ 1.0ìœ¼ë¡œ ì„¤ì •ë˜ê³  Top-K ë° Top-P ì„¤ì •ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤)",
                key=f"{st.session_state['widget_key']}_Model_Reasoning"
            )
            
            if extended_thinking:
                st.info("Model reasoning ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ Temperature ê°’ì´ 1.0ìœ¼ë¡œ ìë™ ì„¤ì •ë˜ê³  Top-K ë° Top-P ì„¤ì •ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                
                show_reasoning = st.checkbox(
                    "Reasoning ê³¼ì • í‘œì‹œ",
                    value=True,
                    help="Claudeì˜ ì‚¬ê³  ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.",
                    key=f"{st.session_state['widget_key']}_Show_Reasoning"
                )
        
        st.markdown("## Document Upload")
        uploaded_file = st.file_uploader(
            "Upload a document",
            type=SUPPORTED_FORMATS,
            help="ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹: pdf, doc, docx, md, ppt, pptx, txt, html, csv, xls, xlsx",
            key=f"{st.session_state['widget_key']}_file_uploader"  # widget_keyì™€ ì—°ë™
        )

        st.markdown("## Inference Parameters")
        system_prompt = st.text_area(
            "System Prompt",
            "You're a helpful assistant who loves to respond in Korean. You'll answer questions based on the uploaded documents, when a document exists. When you're not certain about something, don't guess or make up information. Instead, honestly admit that you don't know.",
            key=f"{st.session_state['widget_key']}_System_Prompt",
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.0,
            step=0.1,
            key=f"{st.session_state['widget_key']}_Temperature",
        )

        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                top_p = st.slider(
                    "Top-P",
                    min_value=0.0,
                    max_value=1.0,
                    value=1.00,
                    step=0.01,
                    key=f"{st.session_state['widget_key']}_Top-P",
                )
            with col2:
                top_k = st.slider(
                    "Top-K",
                    min_value=1,
                    max_value=500,
                    value=500,
                    step=5,
                    key=f"{st.session_state['widget_key']}_Top-K",
                )

        with st.container():
            col1, col2 = st.columns(2)
            with col1:
                model_max_tokens = MODELS[model_name]["max_tokens"]
                max_tokens = st.slider(
                    "Max Token",
                    min_value=0,
                    max_value=model_max_tokens,
                    value=model_max_tokens,
                    step=8,
                    key=f"{st.session_state['widget_key']}_Max_Token",
                )
            with col2:
                memory_window = st.slider(
                    "Memory Window",
                    min_value=0,
                    max_value=10,
                    value=10,
                    step=1,
                    key=f"{st.session_state['widget_key']}_Memory_Window",
                )

    return temperature, top_p, top_k, max_tokens, memory_window, system_prompt, uploaded_file, model_name, extended_thinking, show_reasoning

def process_uploaded_file(file_path: str) -> str:
    """ë¬¸ì„œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    file_ext = os.path.splitext(file_path)[1].lower()
    
    try:
        if file_ext == '.pdf':
            from PyPDF2 import PdfReader
            reader = PdfReader(file_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text
        
        elif file_ext in ['.doc', '.docx']:
            from docx import Document
            doc = Document(file_path)
            text = ""
            for para in doc.paragraphs:
                text += para.text + "\n"
            return text
        
        elif file_ext in ['.txt', '.md', '.html']:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        
        elif file_ext in ['.csv']:
            df = pd.read_csv(file_path)
            return df.to_string()
        
        elif file_ext in ['.xls', '.xlsx']:
            df = pd.read_excel(file_path)
            return df.to_string()
        
        elif file_ext in ['.ppt', '.pptx']:
            from pptx import Presentation
            text = []
            prs = Presentation(file_path)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text.append(shape.text)
            return "\n\n".join(text)
        
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_ext}")
            
    except Exception as e:
        raise Exception(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ ({file_ext}): {str(e)}")

def init_conversation_chain(
    temperature: float,
    top_p: float,
    top_k: int,
    max_tokens: int,
    system_prompt: str,
    model_name: str,
    extended_thinking: bool = False,
) -> Union[ChatBedrock, boto3.client]:

    model_info = MODELS[model_name]
    model_id = model_info["id"]
    use_model_kwargs = model_info["use_model_kwargs"]

    if use_model_kwargs:  # Claude 3.7 Sonnet
        model_kwargs = {
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "max_tokens": max_tokens,
            "system": system_prompt
        }
        
        # Model reasoning ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš° (Claude 3.7 Sonnet ì „ìš©)
        if model_name == "Claude 3.7 Sonnet" and extended_thinking:
            # Model reasoning ëª¨ë“œì—ì„œëŠ” temperatureê°€ ë°˜ë“œì‹œ 1ì´ì–´ì•¼ í•¨
            model_kwargs["temperature"] = 1.0
            model_kwargs["anthropic_version"] = "bedrock-2023-05-31"
            
            # Model reasoning ëª¨ë“œì—ì„œëŠ” top_kì™€ top_pë¥¼ ì„¤ì •í•˜ì§€ ì•Šì•„ì•¼ í•¨
            if "top_k" in model_kwargs:
                del model_kwargs["top_k"]
            if "top_p" in model_kwargs:
                del model_kwargs["top_p"]
            
            # ìµœëŒ€ LengthëŠ” 64000ìœ¼ë¡œ ì„¤ì •
            model_kwargs["max_tokens"] = 64000
            
            # thinking.budget_tokensëŠ” max_tokensë³´ë‹¤ ì‘ì•„ì•¼ í•˜ë©°, ìµœëŒ€ 4096
            thinking_budget = min(4096, model_kwargs["max_tokens"] - 1000)
            
            model_kwargs["thinking"] = {
                "type": "enabled",
                "budget_tokens": thinking_budget  # ì‚¬ê³  ê³¼ì •ì— í• ë‹¹í•  í† í° ìˆ˜
            }
            
        return ChatBedrock(
            model_id=model_id,
            model_kwargs=model_kwargs,
            streaming=True,
            region_name=REGION
        )
    else:  # Nova Pro
        return boto3.client("bedrock-runtime", region_name=REGION)

def convert_langchain_messages_to_anthropic(messages):
    """LangChain ë©”ì‹œì§€ë¥¼ Anthropic API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    result = []
    system_message = None
    
    for msg in messages:
        if msg.type == "system":
            system_message = msg.content  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë³„ë„ ì €ì¥
        elif msg.type == "ai":
            result.append({"role": "assistant", "content": msg.content})
        elif msg.type == "human":
            result.append({"role": "user", "content": msg.content})
    
    return result, system_message

def convert_chat_messages_to_converse_api(chat_messages: List[ChatMessage]) -> List[dict]:
    """ChatMessage ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ Nova Pro API í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    messages = []
    for chat_msg in chat_messages:
        if chat_msg.role != 'system':  # system ì—­í• ì€ ê±´ë„ˆë›°ê¸°
            messages.append({
                "role": chat_msg.role,
                "content": [
                    {
                        "text": chat_msg.text
                    }
                ]
            })
    return messages

def generate_response(
    conversation: Union[ChatBedrock, boto3.client],
    input_text: str,
    chat_history: StreamlitChatMessageHistory,
    show_reasoning: bool = False
) -> str:
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        try:
            if isinstance(conversation, ChatBedrock):  # Claude 3.7 Sonnet
                messages = []
                messages.append(SystemMessage(content=conversation.model_kwargs["system"]))
                for msg in chat_history.messages:
                    messages.append(msg)
                messages.append(HumanMessage(content=input_text))
                
                # Model reasoning ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ í™•ì¸
                has_thinking = "thinking" in conversation.model_kwargs
                
                try:
                    # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ìˆ˜ì •
                    if has_thinking:
                        # Model reasoning ëª¨ë“œì—ì„œëŠ” ì§ì ‘ boto3 í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                        bedrock_runtime = boto3.client("bedrock-runtime", region_name=REGION)
                        
                        # ë©”ì‹œì§€ ë³€í™˜
                        anthropic_messages, system_content = convert_langchain_messages_to_anthropic(messages)
                        
                        # ìµœëŒ€ í† í° ê°’ ì„¤ì • (64000)
                        model_max_tokens = 64000
                        
                        request_payload = {
                            "anthropic_version": "bedrock-2023-05-31",
                            "max_tokens": model_max_tokens,
                            "temperature": 1.0,
                            "system": system_content,  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë³„ë„ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬
                            "thinking": conversation.model_kwargs["thinking"],
                            "messages": anthropic_messages
                        }
                        
                        response = bedrock_runtime.invoke_model_with_response_stream(
                            modelId=MODELS["Claude 3.7 Sonnet"]["id"],
                            body=json.dumps(request_payload)
                        )
                        
                        # ë””ë²„ê¹…ì„ ìœ„í•œ ìš”ì²­ í˜ì´ë¡œë“œ ì¶œë ¥
                        print(f"ìš”ì²­ í˜ì´ë¡œë“œ: {json.dumps(request_payload, indent=2)}")
                        
                        # ì±„íŒ… ë©”ì‹œì§€ ìœ„ì— reasoningì„ í‘œì‹œí•˜ê¸° ìœ„í•´ reasoning_placeholderë¥¼ ë¨¼ì € ìƒì„±
                        reasoning_placeholder = st.empty()
                        
                        reasoning_text = ""
                        has_shown_reasoning = False
                        
                        # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶”ê°€
                        print(f"show_reasoning ê°’: {show_reasoning}")
                        
                        for event in response["body"]:
                            try:
                                chunk = json.loads(event["chunk"]["bytes"])
                                print(f"ì²­í¬ ë°ì´í„°: {chunk}")  # ë””ë²„ê¹…ìš©
                                
                                # thinking íƒ€ì… ì²˜ë¦¬ (reasoning ê³¼ì •)
                                if chunk.get("type") == "thinking":
                                    thinking_content = chunk.get("thinking", "")
                                    reasoning_text += thinking_content
                                    print(f"ì‚¬ê³  ê³¼ì • ê°ì§€: {thinking_content}")
                                    has_shown_reasoning = True
                                    
                                    # reasoning ê³¼ì •ì„ UIì— í‘œì‹œ (show_reasoningì´ Trueì¸ ê²½ìš°ì—ë§Œ)
                                    if show_reasoning:
                                        # ë©”ì‹œì§€ í”Œë ˆì´ìŠ¤í™€ë” ìœ„ì— reasoning í‘œì‹œ
                                        # ë¼ì´íŠ¸/ë‹¤í¬ í…Œë§ˆì— ë”°ë¼ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
                                        is_dark_theme = True  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‹¤í¬ í…Œë§ˆ ê°€ì •
                                        try:
                                            # Streamlit í…Œë§ˆ ê°ì§€ ì‹œë„
                                            theme = st.get_option("theme.base")
                                            is_dark_theme = theme == "dark"
                                        except:
                                            pass  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                                        
                                        bg_color = "#2a2a2a" if is_dark_theme else "#f0f2f6"
                                        text_color = "#ffffff" if is_dark_theme else "#262730"
                                        
                                        reasoning_placeholder.markdown(f"""
                                        <div style="background-color: {bg_color}; color: {text_color}; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #007bff; border: 1px solid #007bff; max-height: 400px; overflow-y: auto;">
                                            <h4 style="color: {text_color}; margin-top: 0;">ğŸ§  Reasoning...</h4>
                                            <pre style="white-space: pre-wrap; overflow-wrap: break-word; color: {text_color}; background-color: transparent; border: none; padding: 0; margin: 0;">{reasoning_text}</pre>
                                        </div>
                                        """, unsafe_allow_html=True)
                                        print("Reasoning UI ì—…ë°ì´íŠ¸ë¨")
                                
                                # content_block_delta íƒ€ì… ì²˜ë¦¬ - text ë˜ëŠ” thinking_delta ëª¨ë‘ ì²˜ë¦¬
                                elif chunk.get("type") == "content_block_delta":
                                    # text ë˜ëŠ” text_delta ì²˜ë¦¬
                                    if chunk["delta"].get("type") == "text" or chunk["delta"].get("type") == "text_delta":
                                        text_chunk = chunk["delta"].get("text", "")
                                        full_response += text_chunk
                                        print(f"í˜„ì¬ ì‘ë‹µ: {full_response}")  # ì‘ë‹µ ë‚´ìš© í™•ì¸
                                        
                                        # ë§¤ ì²­í¬ë§ˆë‹¤ ì—…ë°ì´íŠ¸í•˜ì§€ ë§ê³  ì¼ì • ê°„ê²©ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                                        if len(text_chunk) > 10 or text_chunk.endswith(('.', '!', '?', '\n')):
                                            message_placeholder.markdown(full_response + "â–Œ")
                                    
                                    # thinking_delta ì²˜ë¦¬ (ì‚¬ê³  ê³¼ì •)
                                    elif show_reasoning and chunk["delta"].get("type") == "thinking_delta":
                                        thinking_chunk = chunk["delta"].get("thinking", "")
                                        if thinking_chunk:
                                            reasoning_text += thinking_chunk
                                            print(f"thinking_delta ì‚¬ê³  ê³¼ì •: {thinking_chunk}")
                                            has_shown_reasoning = True
                                            
                                            # reasoning ê³¼ì •ì„ UIì— í‘œì‹œ
                                            # ë¼ì´íŠ¸/ë‹¤í¬ í…Œë§ˆì— ë”°ë¼ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
                                            is_dark_theme = True  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‹¤í¬ í…Œë§ˆ ê°€ì •
                                            try:
                                                # Streamlit í…Œë§ˆ ê°ì§€ ì‹œë„
                                                theme = st.get_option("theme.base")
                                                is_dark_theme = theme == "dark"
                                            except:
                                                pass  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                                            
                                            bg_color = "#2a2a2a" if is_dark_theme else "#f0f2f6"
                                            text_color = "#ffffff" if is_dark_theme else "#262730"
                                            
                                            reasoning_placeholder.markdown(f"""
                                            <div style="background-color: {bg_color}; color: {text_color}; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #007bff; border: 1px solid #007bff; max-height: 400px; overflow-y: auto;">
                                                <h4 style="color: {text_color}; margin-top: 0;">ğŸ§  Reasoning...</h4>
                                                <pre style="white-space: pre-wrap; overflow-wrap: break-word; color: {text_color}; background-color: transparent; border: none; padding: 0; margin: 0;">{reasoning_text}</pre>
                                            </div>
                                            """, unsafe_allow_html=True)
                                
                                # ë‹¤ë¥¸ í˜•ì‹ì˜ thinking ë°ì´í„° ì²˜ë¦¬ ì‹œë„
                                elif "thinking" in chunk:
                                    thinking_content = chunk.get("thinking", "")
                                    reasoning_text += thinking_content
                                    print(f"ë‹¤ë¥¸ í˜•ì‹ì˜ ì‚¬ê³  ê³¼ì • ê°ì§€: {thinking_content}")
                                    has_shown_reasoning = True
                                    
                                    # reasoning ê³¼ì •ì„ UIì— í‘œì‹œ (show_reasoningì´ Trueì¸ ê²½ìš°ì—ë§Œ)
                                    if show_reasoning:
                                        # ë¼ì´íŠ¸/ë‹¤í¬ í…Œë§ˆì— ë”°ë¼ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
                                        is_dark_theme = True  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‹¤í¬ í…Œë§ˆ ê°€ì •
                                        try:
                                            # Streamlit í…Œë§ˆ ê°ì§€ ì‹œë„
                                            theme = st.get_option("theme.base")
                                            is_dark_theme = theme == "dark"
                                        except:
                                            pass  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                                        
                                        bg_color = "#2a2a2a" if is_dark_theme else "#f0f2f6"
                                        text_color = "#ffffff" if is_dark_theme else "#262730"
                                        
                                        reasoning_placeholder.markdown(f"""
                                        <div style="background-color: {bg_color}; color: {text_color}; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #007bff; border: 1px solid #007bff; max-height: 400px; overflow-y: auto;">
                                            <h4 style="color: {text_color}; margin-top: 0;">ğŸ§  Reasoning...</h4>
                                            <pre style="white-space: pre-wrap; overflow-wrap: break-word; color: {text_color}; background-color: transparent; border: none; padding: 0; margin: 0;">{reasoning_text}</pre>
                                        </div>
                                        """, unsafe_allow_html=True)
                                        print("Reasoning UI ì—…ë°ì´íŠ¸ë¨ (ë‹¤ë¥¸ í˜•ì‹)")
                                
                                # content_block_start íƒ€ì… ì²˜ë¦¬ (reasoning ê³¼ì •ì´ ì—¬ê¸°ì— í¬í•¨ë  ìˆ˜ ìˆìŒ)
                                elif chunk.get("type") == "content_block_start" and "thinking" in str(chunk):
                                    try:
                                        # ë‹¤ì–‘í•œ í˜•ì‹ì˜ thinking ë°ì´í„° ì¶”ì¶œ ì‹œë„
                                        thinking_content = ""
                                        
                                        # content_block ë‚´ì— thinking í•„ë“œê°€ ìˆëŠ” ê²½ìš°
                                        if "content_block" in chunk and "thinking" in chunk["content_block"]:
                                            thinking_content = chunk["content_block"].get("thinking", "")
                                        # ì§ì ‘ thinking í•„ë“œê°€ ìˆëŠ” ê²½ìš°
                                        elif "thinking" in chunk:
                                            thinking_content = chunk.get("thinking", "")
                                        
                                        # ì¶”ì¶œëœ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                                        if thinking_content and not isinstance(thinking_content, dict):
                                            reasoning_text += thinking_content
                                            print(f"content_block_startì—ì„œ ì‚¬ê³  ê³¼ì • ê°ì§€: {thinking_content}")
                                            has_shown_reasoning = True
                                            
                                            # reasoning ê³¼ì •ì„ UIì— í‘œì‹œ (show_reasoningì´ Trueì¸ ê²½ìš°ì—ë§Œ)
                                            if show_reasoning:
                                                # ë¼ì´íŠ¸/ë‹¤í¬ í…Œë§ˆì— ë”°ë¼ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
                                                is_dark_theme = True  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‹¤í¬ í…Œë§ˆ ê°€ì •
                                                try:
                                                    # Streamlit í…Œë§ˆ ê°ì§€ ì‹œë„
                                                    theme = st.get_option("theme.base")
                                                    is_dark_theme = theme == "dark"
                                                except:
                                                    pass  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                                                
                                                bg_color = "#2a2a2a" if is_dark_theme else "#f0f2f6"
                                                text_color = "#ffffff" if is_dark_theme else "#262730"
                                                
                                                reasoning_placeholder.markdown(f"""
                                                <div style="background-color: {bg_color}; color: {text_color}; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #007bff; border: 1px solid #007bff; max-height: 400px; overflow-y: auto;">
                                                    <h4 style="color: {text_color}; margin-top: 0;">ğŸ§  Reasoning...</h4>
                                                    <pre style="white-space: pre-wrap; overflow-wrap: break-word; color: {text_color}; background-color: transparent; border: none; padding: 0; margin: 0;">{reasoning_text}</pre>
                                                </div>
                                                """, unsafe_allow_html=True)
                                                print("Reasoning UI ì—…ë°ì´íŠ¸ë¨ (content_block_start)")
                                    except Exception as thinking_error:
                                        print(f"ì‚¬ê³  ê³¼ì • ì¶”ì¶œ ì˜¤ë¥˜: {str(thinking_error)}")
                                
                                # ëª¨ë“  ì²­í¬ì—ì„œ "thinking" ë¬¸ìì—´ì„ ì°¾ì•„ ì²˜ë¦¬ (ë§ˆì§€ë§‰ ì‹œë„)
                                elif show_reasoning and "thinking" in str(chunk).lower():
                                    try:
                                        # JSON ë°ì´í„°ì—ì„œ ì‹¤ì œ ì‚¬ê³  ê³¼ì • ë‚´ìš©ë§Œ ì¶”ì¶œ
                                        chunk_data = chunk
                                        thinking_content = ""
                                        
                                        # content_block_delta íƒ€ì…ì¸ ê²½ìš°
                                        if isinstance(chunk_data, dict) and chunk_data.get("type") == "content_block_delta":
                                            if "delta" in chunk_data and "thinking" in chunk_data["delta"]:
                                                thinking_content = chunk_data["delta"].get("thinking", "")
                                        
                                        # ë‹¤ë¥¸ í˜•ì‹ì˜ thinking ë°ì´í„° ì²˜ë¦¬
                                        elif isinstance(chunk_data, dict) and "thinking" in chunk_data:
                                            thinking_content = chunk_data.get("thinking", "")
                                        
                                        # ì¶”ì¶œëœ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                                        if thinking_content:
                                            reasoning_text += thinking_content
                                            print(f"ì‚¬ê³  ê³¼ì • ë‚´ìš© ì¶”ì¶œ: {thinking_content}")
                                            has_shown_reasoning = True
                                            
                                            # reasoning ê³¼ì •ì„ UIì— í‘œì‹œ
                                            # ë¼ì´íŠ¸/ë‹¤í¬ í…Œë§ˆì— ë”°ë¼ ë‹¤ë¥¸ ìƒ‰ìƒ ì‚¬ìš©
                                            is_dark_theme = True  # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‹¤í¬ í…Œë§ˆ ê°€ì •
                                            try:
                                                # Streamlit í…Œë§ˆ ê°ì§€ ì‹œë„
                                                theme = st.get_option("theme.base")
                                                is_dark_theme = theme == "dark"
                                            except:
                                                pass  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
                                            
                                            bg_color = "#2a2a2a" if is_dark_theme else "#f0f2f6"
                                            text_color = "#ffffff" if is_dark_theme else "#262730"
                                            
                                            reasoning_placeholder.markdown(f"""
                                            <div style="background-color: {bg_color}; color: {text_color}; padding: 10px; border-radius: 5px; margin-bottom: 10px; border-left: 4px solid #007bff; border: 1px solid #007bff; max-height: 400px; overflow-y: auto;">
                                                <h4 style="color: {text_color}; margin-top: 0;">ğŸ§  Reasoning...</h4>
                                                <pre style="white-space: pre-wrap; overflow-wrap: break-word; color: {text_color}; background-color: transparent; border: none; padding: 0; margin: 0;">{reasoning_text}</pre>
                                            </div>
                                            """, unsafe_allow_html=True)
                                            print("Reasoning UI ì—…ë°ì´íŠ¸ë¨ (ë‚´ìš© ì¶”ì¶œ)")
                                    except Exception as thinking_error:
                                        print(f"ì‚¬ê³  ê³¼ì • ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {str(thinking_error)}")
                                        
                                # ì‘ë‹µ ì™„ë£Œ ì´ë²¤íŠ¸ ì²˜ë¦¬
                                elif chunk.get("type") == "message_stop":
                                    print("ì‘ë‹µ ì™„ë£Œ")
                                    message_placeholder.markdown(full_response)
                            except Exception as chunk_error:
                                print(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(chunk_error)}")
                        
                        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… ë©”ì‹œì§€ í‘œì‹œ
                        print(f"ìµœì¢… ì‘ë‹µ: {full_response}")
                        message_placeholder.markdown(full_response)
                        
                        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
                        print(f"reasoning í‘œì‹œ ì—¬ë¶€: {has_shown_reasoning}, show_reasoning ê°’: {show_reasoning}")
                        if not has_shown_reasoning and show_reasoning:
                            print("ê²½ê³ : reasoning ë°ì´í„°ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    else:
                        # ì¼ë°˜ ëª¨ë“œì—ì„œëŠ” LangChain ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
                        for chunk in conversation.stream(messages):
                            if hasattr(chunk, 'content') and chunk.content:
                                full_response += chunk.content
                                message_placeholder.markdown(full_response + "â–Œ")
                    
                    message_placeholder.markdown(full_response)
                    return full_response
                except Exception as e:
                    st.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ ì‹œ ì¼ë°˜ í˜¸ì¶œ ì‹œë„
                    try:
                        response = conversation.invoke(messages)
                        full_response = response.content
                        message_placeholder.markdown(full_response)
                        return full_response
                    except Exception as e2:
                        st.error(f"ì¼ë°˜ í˜¸ì¶œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e2)}")
                        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e2)}"
            else:  # Nova Pro
                # ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
                new_message = ChatMessage('user', input_text)
                st.session_state.nova_messages.append(new_message)
                
                # ë©”ì‹œì§€ ìˆ˜ ì œí•œ í™•ì¸
                if len(st.session_state.nova_messages) > MAX_MESSAGES * 2:  # userì™€ assistant ë©”ì‹œì§€ ìŒì„ ê³ ë ¤
                    st.session_state.nova_messages = st.session_state.nova_messages[-(MAX_MESSAGES * 2):]
                
                # Nova Pro API í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ë³€í™˜
                messages = convert_chat_messages_to_converse_api(st.session_state.nova_messages)
                
                # Nova Pro íŒŒë¼ë¯¸í„° ì„¤ì •
                widget_key = st.session_state["widget_key"]
                
                response = conversation.converse_stream(
                    modelId=MODELS["Nova Pro 1.0"]["id"],
                    messages=messages,
                    inferenceConfig={
                        "temperature": float(st.session_state[f"{widget_key}_Temperature"]),
                        "maxTokens": int(st.session_state[f"{widget_key}_Max_Token"]),
                    }
                )

                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                for event in response['stream']:
                    if 'contentBlockDelta' in event:
                        chunk = event['contentBlockDelta']['delta']['text']
                        full_response += chunk
                        message_placeholder.markdown(full_response + "â–Œ")
                
                message_placeholder.markdown(full_response)
                
                # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì €ì¥
                st.session_state.nova_messages.append(ChatMessage('assistant', full_response))
                return full_response

        except Exception as e:
            error_detail = str(e)
            print(f"ìƒì„¸ ì˜¤ë¥˜: {error_detail}")  # í„°ë¯¸ë„ì— ìì„¸í•œ ì˜¤ë¥˜ ì¶œë ¥
            
            if "ThrottlingException" in error_detail:
                error_message = "ìš”ì²­ì„ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ™"
            elif "'message'" in error_detail:
                error_message = "ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. Model reasoning ëª¨ë“œ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            elif "validationException" in error_detail:
                error_message = "API ê²€ì¦ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìš”ì²­ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
                print(f"API ê²€ì¦ ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
            else:
                error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error_detail}"
            
            message_placeholder.markdown(error_message)
            return error_message

def new_chat() -> None:
    """ìƒˆë¡œìš´ ëŒ€í™” ì‹œì‘"""
    # widget_keyë¥¼ ìƒˆë¡œ ìƒì„±í•˜ì—¬ file_uploaderë¥¼ í¬í•¨í•œ ëª¨ë“  ìœ„ì ¯ ì´ˆê¸°í™”
    st.session_state["widget_key"] = str(random.randint(1, 1000000))
    st.session_state.messages = []
    st.session_state.chat_history.clear()
    st.session_state.nova_messages = []
    st.session_state.context = None
    st.session_state.initial_system_message = None

def handle_file_upload(uploaded_file) -> str:
    """íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
    if uploaded_file:
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(uploaded_file.name)[1]) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        try:
            document_text = process_uploaded_file(tmp_file_path)
            st.session_state.context = document_text
            os.unlink(tmp_file_path)
            return document_text
        except Exception as e:
            os.unlink(tmp_file_path)
            st.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return None
    return None

def main() -> None:
    set_page_config()
    initialize_session_state()

    st.sidebar.button("New Chat", on_click=new_chat, type="primary")

    temperature, top_p, top_k, max_tokens, memory_window, system_prompt, uploaded_file, model_name, extended_thinking, show_reasoning = get_sidebar_params()

    # ë¬¸ì„œê°€ ì—…ë¡œë“œë˜ë©´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì´ˆê¸°í™”
    if uploaded_file:
        document_context = handle_file_upload(uploaded_file)
        if document_context:
            st.sidebar.success(f"ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {uploaded_file.name}")
            # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±
            full_system_prompt = f"{system_prompt}\n\nì°¸ê³ í•  ë¬¸ì„œ ë‚´ìš©:\n\n{document_context}"
            st.session_state.initial_system_message = full_system_prompt
            
            # Nova Proì˜ ê²½ìš° ì´ˆê¸° ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ user ë©”ì‹œì§€ë¡œ ì¶”ê°€
            if model_name == "Nova Pro 1.0" and not st.session_state.nova_messages:
                st.session_state.nova_messages = [
                    ChatMessage('user', full_system_prompt),
                    ChatMessage('assistant', "ë„¤, ì´í•´í–ˆìŠµë‹ˆë‹¤. ì—…ë¡œë“œëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.")
                ]

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if st.session_state.initial_system_message and model_name == "Claude 3.7 Sonnet":
        system_prompt = st.session_state.initial_system_message

    conv_chain = init_conversation_chain(
        temperature, top_p, top_k, max_tokens, system_prompt, model_name, extended_thinking
    )

    # ì €ì¥ëœ ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.session_state.chat_history.add_user_message(prompt)
        
        with st.chat_message("user"):
            st.markdown(prompt)

        # ì‘ë‹µ ìƒì„±
        response = generate_response(conv_chain, prompt, st.session_state.chat_history, show_reasoning)
        
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.session_state.chat_history.add_ai_message(response)

if __name__ == "__main__":
    main()
